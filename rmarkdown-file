---
title: "BT2103 Project"
date: "2023-03-23"
output:
  pdf_document: default
  html_document:
    df_print: paged
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)
```


```{r data_library, include = FALSE}
library(dplyr)
library(readxl)
library(moments)
library(corrplot)
library(kableExtra)
library(tidyr)
library(caret)
library(data.table)
library(mltools)
library(ROSE)
library(leaps)
library(nnet)
library(randomForest)
library(ROCit)
library(e1071)

data <- read.table("card.csv",sep=",",skip=2, header = FALSE)
header <- scan("card.csv",sep=",",nlines=2,what=character())
set.seed(9999)
n = length(data$V1)

```

## 1. Introduction of dataset & Modelling of the problem

The data set consists of 30,000 credit card holders information which is obtained from a bank in Taiwan. There are 23 feature attributes (V2 - V24) and 1 target variable which are further explained below. The target feature (V25) is predicted to be a binary value 0 (= not default) or 1 (= default).

This study reviewed the literature and used the following 23 variables as explanatory variables:  

* ID (V1): Indexation/ID of each individual (From 1 to 30,000).

* LIMIT_BAL (V2): Amount of the given credit in NT dollar. It includes both the individual consumer credit and his/her family (supplementary) credit. 

* SEX (V3): Gender of the individual (1 = male; 2 = female).  

* EDUCATION (V4): Education level of the individual (1 = graduate school; 2 = university; 3 = high school; 4 = others).  

* MARRIAGE (V5): Marital status of the individual (1 = married; 2 = single; 3 = others).  

* AGE (V6): Age of the individual in terms of years.  

* PAY_0, PAY_2 to PAY_6 (V7 - V12): History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: 
PAY_0 (V7) = the repayment status in September, 2005; PAY_2 (V8) = the repayment status in August, 2005; . . .;PAY_6 (V12) = the repayment status in April, 2005. The measurement scale for the repayment status is: -2 = no consumption; -1 = pay duly; 0 = the use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.  

* BILL_AMT1 to BILL_AMT6 (V13-V18): Amount of bill statement in NT dollar. BILL_AMT1 (V13) = amount of bill statement in September, 2005; BILL_AMT2 (V14) = amount of bill statement in August, 2005; . . .; BILL_AMT6 (V18) = amount of bill statement in April, 2005.  

* PAY_AMT1 to PAY_AMT6 (V19-V24): Amount of previous payment in NT dollar. PAY_AMT1 (V19) = amount paid in September, 2005; PAY_AMT2 (V20) = amount paid in August, 2005; . . .; PAY_AMT6 (V24) = amount paid in April, 2005.

With the given data set and features attributes, we want to seek out ways to classify our credit card holders as either a defaulter or a non-defaulter so that the bank can make appropriate decision to approve the loan, depending on the classification. This is because the bank does not want to misclassify a customer to be a non-defaulter when he is a defaulter, which would result in losses for the bank as they would find it difficult to recover any money. Conversely, the bank does not want to misclassify a customer to be a defaulter when he is not, as it would mean that they would not be approving the loan and hence losing money that could be earned from the interest. Therefore, it is important to correctly classify the customer in order to maximise their profits and reduce their losses.

## 2.1 Exploratory data analysis

From the initial overview of the data set, we can see that although there are no missing values in any of the observations, there are some values that seems to differ from the supposed range of values. For example, under *EDUCATION (V4)* the supposed range of values is from 1 to 4. However, we observe that there are 14, 280 and 51 observations with value of 0, 5 and 6 respectively. Since there is already an existing value "4" that corresponds to "others", we will reclassify these observations under "others" instead to retain as many observations as possible. 

```{r education, echo=FALSE}
V4 <- data %>% group_by(V4) %>% count
kable(V4, caption = "Distribution of Education (V4)") %>%
  kable_styling(latex_options = "HOLD_position")

data <- data %>% mutate(V4 = ifelse(V4 == 0 | V4 == 5 | V4 == 6, 4, V4))
```
Similarly under *MARRIAGE (V5)* the supposed range of values is from 1 to 3. However, we observe that there are 54 observations with value of 0. For these observations, Since there is already an existing value "3" that corresponds to "others", we will reclassify these observations under "others" instead to retain as many observations as possible. 

```{r marriage, echo=FALSE}
V5 <- data %>% group_by(V5) %>% count
kable(V5, caption = "Distribution of Marital Status (V5)") %>%
  kable_styling(latex_options = "HOLD_position")

data <- data %>% mutate(V5 = ifelse(V5 == 0, 3, V5))
```

Furthermore, we also observed that for *BILLAMT1 to BILLAMT6 (V13 to V18)* there are observations with negative numbers. We found that there are a total of 1930 observations with negative bill statement, and we treat it as the possibility of refund to credit card holders from the bank which therefore results in negative bill amount and hence decided not to remove or modify these observations. 

```{r billamt, echo = FALSE}
negativebillamt <- data %>% filter(V13 < 0 | V14 < 0 | V15 < 0 | V16 < 0 | V17 < 0 | V18 < 0) %>% count

kable(negativebillamt, caption = "Frequency of observation with negative bill amount") %>%
  kable_styling(latex_options = "HOLD_position")

```


## 2.2 Visualisation of Observations

We will also be analyzing the different attributes to discover any patterns or uneven balance of observations among the attribute against the possibility of a being a defaulter.

Firstly we will look at the distribution of default status within our data set to see if it is a balance data set or not. We observed that the data set is unbalanced as among all the observations, 77.9% of the observations are indicated as non-defaulter status and only 22.1% of the observations are indicated as defaulter status. 

```{r default_status, echo = FALSE, fig.asp=0.50, fig.align='center'}
#Pie chart for default and non-defaulter
defaultFreq <- data %>% count(V25)
defaultFreq$Percentage <- NA
totalSum <- sum(defaultFreq$n)
for (i in 1:length(defaultFreq)-1) {
  defaultFreq$Percentage[i] = defaultFreq$n[i]/totalSum * 100
}

kable(defaultFreq, caption = "Frequency of Credit Card Holder based on Default Status (0 = Non-defaulter, 1 = Defaulter)") %>% kable_styling(latex_options = "HOLD_position")

```


Secondly, we will look at the amount of given credit to the credit card holder as well as the family's credit. We observe that majority of the customers falls between "0 to 500,000" credit given, with most customers, 7676 of them, receiving "0 to 50,000" credit. Although there is a customer that is given "1,000,000" credit which differs greatly from most credit card holders, it maybe due to other factors which awards him/her a lot more credit and hence, we will not deem it as an anomaly.

```{r credit_value, include = FALSE}
#credit histogram
options(scipen=999)
h.credit <-hist(data$V2, 
           main="Histogram of Credit given to Customers",
           xlab="Amount of Credit given",
           ylab="No. of Customers",
           col=c("darkorange") ,
           ylim = c(0,8000),
           labels=TRUE)

```

```{r creditgraph, echo = FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("/Users/jett/Desktop/Y2S2/BT2103/proj/histogram_credit.png")
```
Among the different bin grouping in the histogram, we further sub-divided the observations based on the default status of the credit card holder. We observe that majority of the defaulter and non-defaulter are also found in the "0 to 50,000" grouping and for the remaining groupings, the proportion of non-default status is more than the default status, due to the imbalance data set with regards to the default status.

```{r credit_default, include = FALSE}
#creating the data for credit & default
credit <- data %>% select(V2, V25) %>% mutate(V2 = ifelse(V2 >0 & V2 <=50000, 1, ifelse(V2 > 50000 & V2 <= 100000, 2, ifelse(V2 > 100000 & V2 <= 150000, 3, ifelse(V2 > 150000 & V2 <= 200000, 4, ifelse(V2 > 200000 & V2 <= 250000, 5, ifelse(V2 > 250000 & V2 <= 300000, 6, ifelse(V2 > 300000 & V2 <= 350000, 7, ifelse(V2 > 350000 & V2 <= 400000, 8, ifelse(V2 > 400000 & V2 <= 450000, 9, ifelse(V2 > 450000 & V2 <= 500000, 10, ifelse(V2 > 500000 & V2 <= 550000, 11, ifelse(V2 > 550000 & V2 <= 600000, 12, ifelse(V2 > 600000 & V2 <= 650000, 13, ifelse(V2 > 650000 & V2 <= 700000, 14, ifelse(V2 > 700000 & V2 <= 750000, 15, ifelse(V2 > 750000 & V2 <= 800000, 16, ifelse(V2 >800000 & V2 <= 850000, 17, ifelse( V2 > 850000 & V2 <= 900000, 18, ifelse(V2>900000 & V2<=950000, 19, 20)))))))))))))))))))) %>% group_by(V2, V25) %>% count

credit.spread<- credit %>% spread(key=V2,value=n)
barmatrix.credit.spread<-as.matrix(credit.spread[,c(2:9)]) 
barmatrix.credit.spread2<-as.matrix(credit.spread[,c(10:18)]) 
bar_Col1<-c("green","red")

barplot(barmatrix.credit.spread, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Credit and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0,6000),
        beside=TRUE,
        cex.names = 0.5)
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))

barplot(barmatrix.credit.spread2, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Credit and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0,1000),
        beside=TRUE,
        cex.names = 0.5)
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))


```

```{r credit_graph, echo = FALSE, out.width="45%", fig.align='center', fig.show = 'hold'}
knitr::include_graphics(c("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_credit1.png","/Users/jett/Desktop/Y2S2/BT2103/proj/graph_credit2.png"))
```

On further inspection, we can also observe that although gender is relatively balance with 60.4% female and 39.6% male credit card holders. In addition, there is a higher proportion of defaulter status at 24.2% for the male credit card holders as compared to 20.8% for the female credit card holders. 

```{r gender, echo = FALSE}
genderFreq <- data %>% count(V3)
genderFreq$Percentage <- NA
totalSum <- sum(genderFreq$n)
for (i in 1:length(genderFreq)-1) {
  genderFreq$Percentage[i] = genderFreq$n[i]/totalSum * 100
}

kable(genderFreq, caption = "Frequency of Credit Card Holder by Gender (1 = Male, 2 = Female)") %>% kable_styling(latex_options = "hold_position")
```

```{r gender_default, include = FALSE}
#barplot for gender and defaulter 
gender <- data %>% group_by(V3, V25) %>% tally()
gender.spread<- gender %>% spread(key=V3,value=n)
gender.spread <- gender.spread %>% rename("Male" = "1", "Female" = "2")
barmatrix.gender<-as.matrix(gender.spread[,c(2:3)])
bar_Col1<-c("green","red")

barplot(barmatrix.gender, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Gender and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0, 20000),
        beside=TRUE) 
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))
```

```{r gender_graph, echo = FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_gender.png")
```

However for education level, we observe that most of the credit card holders are either from Graduate School or University which accounts for 35.2% and 46.8% respectively. When looking at the default status, credit card holders with "Others" as their indicated educational level actually have the most defaulter percentage at 25.1% although it is the least common educational level for the credit card holders. This is in comparison to University and Graduate School educational level which have a proportion of defaulter status of 23.7% and 19.2% respectively.

```{r eduaction, include = FALSE}
#bargraph for education level
education_level <- data %>% group_by(V4) %>% count %>% mutate(V4 = ifelse(V4 == "1", "Graduate School", ifelse( V4 == "2", "University", ifelse(V4 == "3", "High School", "Others"))))
kable(education_level, caption = "Frequency of Credit Card Holders by Educational Level")
educationbp <- barplot(education_level$n, names.arg=education_level$V4, col="skyblue", main="Frequency of Credit Card Holder by Educational Level", xlab="Education Level", ylim =c(0,16000), las=1) 
text(x=educationbp, y=education_level$n, col="black", education_level$n, pos=3)
```

```{r education_default, include = FALSE}
#barplot graph for education and default status
education <- data %>% group_by(V4, V25) %>% tally()
education.spread<- education %>% spread(key=V4,value=n)
education.spread <- education.spread %>% rename("Graduate School" = "1", "University" = "2", "High School" = "3", "Others" = "4")
barmatrix.education<-as.matrix(education.spread[,c(2:5)]) 
bar_Col1<-c("green","red")

barplot(barmatrix.education, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Education and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0, 12000),
        beside=TRUE)
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))

```

```{r education_graph, echo = FALSE, out.width="45%", fig.align='center', fig.show = 'hold'}
knitr::include_graphics(c("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_education.png", "/Users/jett/Desktop/Y2S2/BT2103/proj/graph_education2.png"))
```

Meanwhile for marital status, it is relatively balanced with 45.5% of the credit card holder being Married, 53.2% of them being Single and a small minority of 1.3% being others. Similarly for the defaulter status among the 3 martial status, the proportion is relatively the same being 23.5%, 20.9% and 23.6% for Married, Single and Others marital status respectively. 

```{r marital, include = FALSE}
#bar graph for marriage
marriage_status <- data %>% group_by(V5) %>% count %>% mutate(V5 = ifelse(V5 == "1", "Married", ifelse(V5 == "2", "Single", "Others")))
kable(marriage_status, caption = "Frequency of Credit Card Holders by Marriage Status")
marriagebp <- barplot(marriage_status$n, names.arg=marriage_status$V5, col="skyblue", main="Frequency of Credit Card Holder by Marital Status", xlab="Marital Status", las=1, ylim=c(0,20000)) 
text(x=marriagebp, y=marriage_status$n, col="black", marriage_status$n, pos=3)

```

```{r marital_default, include = FALSE}
#barplot for marriage and default status
marital <- data %>% group_by(V5, V25) %>% tally()
marital.spread<- marital %>% spread(key=V5,value=n)
marital.spread <- marital.spread %>% rename("Married" = "1", "Single" = "2", "Others" = "3")
barmatrix.marital<-as.matrix(marital.spread[,c(2:4)]) 
bar_Col1<-c("green","red")

barplot(barmatrix.marital, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Marital and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0, 14000),
        beside=TRUE)
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))

```

```{r marital_graph, echo = FALSE, out.width="45%", fig.align='center', fig.show = 'hold'}
knitr::include_graphics(c("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_marital.png","/Users/jett/Desktop/Y2S2/BT2103/proj/graph_marital2.png"))
```

We observe that most credit card holders are between the age of 20 to 55. When comparing the age and the default status, we can observe that majority of the defaulters are between the age 20 to 50 and less are observed in the age group of 51 to 80. This could be caused by their lifestyle habits and expenses, which is higher for younger generations. 

```{r age, include = FALSE}
#age histogram
h.age<-hist(data$V6, 
           main="Histogram of Credit Card Holder Age",
           xlab="Age of Credit Card Holder",
           ylab="No. of Credit Card Holder",
           col=c("darkorange") ,
           ylim = c(0,8000),
           labels=TRUE)

# extract frequency table from hist()
Age.Group<-cut(data$V6,h.age$breaks)
t.emp<-table(Age.Group)
kable(t.emp, caption = "Frequency distribution by Age")
```

```{r age_table, echo = FALSE}
kable(t.emp, caption = "Frequency distribution by Age") %>% kable_styling(latex_options = "HOLD_position")
```

```{r age_graph, echo = FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_age.png")
```

```{r age_default, include = FALSE}
#barplot for age and default status
age <- data %>% select(V6, V25) %>% mutate(V6 = ifelse(V6 >20 & V6 <=25, 1, ifelse(V6 > 25 & V6 <= 30, 2, ifelse(V6 > 30 & V6 <= 35, 3, ifelse(V6 > 35 & V6 <= 40, 4, ifelse(V6 > 40 & V6 <= 45, 5, ifelse(V6 > 45 & V6 <= 50, 6, ifelse(V6 > 50 & V6 <= 55, 7, ifelse(V6 > 55 & V6 <= 60, 8, ifelse(V6 > 60 & V6 <= 65, 9, ifelse(V6 > 65 & V6 <= 70, 10, ifelse(V6 > 70 & V6 <= 75, 11, 12)))))))))))) %>% group_by(V6, V25) %>% count

age.spread<- age %>% spread(key=V6,value=n)
age.spread <- age.spread %>% rename("Age 21 to 25" = "1", "Age 26 to 30" = "2", "Age 31 to 35" = "3", "Age 36 to 40" = "4", "Age 41 to 45" = "5", "Age 46 to 50" = "6", "Age 51 to 55" = "7", "Age 56 to 60" = "8", "Age 61 to 65" = "9", "Age 65 to 70" = "10", "Age 71 to 75" = "11", "Age 75 to 80" = "12")
barmatrix.age.spread<-as.matrix(age.spread[,c(2:7)]) 
bar_Col1<-c("green","red")

barplot(barmatrix.age.spread, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Age and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0, 7000),
        beside=TRUE,
        cex.names = 0.5)
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))

barmatrix.age.spread2 <-as.matrix(age.spread[,c(8:13)]) 
barplot(barmatrix.age.spread2, 
        col=bar_Col1,  
        main="Frequncy of Credit Card holder by Age and Default Status",
        ylab="No. of Credit Card holder", 
        ylim = c(0, 1200),
        beside=TRUE,
        cex.names = 0.5)
legend("topright", cex=0.6, fill=bar_Col1, legend=c("Non-Defaulter","Defaulter"))
```

```{r age_graph_2, echo = FALSE, out.width="48%", fig.align='center', fig.show='hold'}
knitr::include_graphics(c("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_age2.png","/Users/jett/Desktop/Y2S2/BT2103/proj/graph_age3.png"))
```


We can also observe that among the different possible observation result for history of past payment, over 50% of the observations for most of the states ("2" to "8") in each of the variable (V7 to V12) are defaulter. This is no surprise because for observed state "2" to "8", it means that the credit card holder have at least delayed their payments for 2 months. Hence, the fact that they are already delaying their credit payments is likely because they are unable to pay back the amount, rather than forgetting about the payment. Therefore, the odds of them being a defaulter is much higher.


```{r payment_default, echo = FALSE}
V7 <- data %>% group_by(V7, V25) %>% count %>% spread(key = V25, value = n)
V7$Proportion <- NA
for (i in 1:nrow(V7)) {
  V7$Proportion[i] = V7$`1`[i]/(V7$`1`[i] + V7$`0`[i])
}

V8 <- data %>% group_by(V8, V25) %>% count %>% spread(key = V25, value = n)
V8$Proportion <- NA
for (i in 1:nrow(V8)) {
  V8$Proportion[i] = V8$`1`[i]/(V8$`1`[i] + V8$`0`[i])
}

V9 <- data %>% group_by(V9, V25) %>% count %>% spread(key = V25, value = n)
V9$Proportion <- NA
for (i in 1:nrow(V9)) {
  V9$Proportion[i] = V9$`1`[i]/(V9$`1`[i] + V9$`0`[i])
}

V10 <- data %>% group_by(V10, V25) %>% count %>% spread(key = V25, value = n)
V10$Proportion <- NA
for (i in 1:nrow(V10)) {
  V10$Proportion[i] = V10$`1`[i]/(V10$`1`[i] + V10$`0`[i])
}

V11 <- data %>% group_by(V11, V25) %>% count %>% spread(key = V25, value = n)
V11$Proportion <- NA
for (i in 1:nrow(V11)) {
  V11$Proportion[i] = V11$`1`[i]/(V11$`1`[i] + V11$`0`[i])
}

V12 <- data %>% group_by(V12, V25) %>% count %>% spread(key = V25, value = n)
V12$Proportion <- NA
for (i in 1:nrow(V12)) {
  V12$Proportion[i] = V12$`1`[i]/(V12$`1`[i] + V12$`0`[i])
}

#V11 & V12 doesnt have "1" in their data so need to reformat
V11_data <- NA
V11_data[1:3] <- V11$Proportion[c(1:3)]
V11_data[4] <- 0 
V11_data[5:11] <- V11$Proportion[c(4:10)]

V12_data <- NA
V12_data[1:3] <- V12$Proportion[c(1:3)]
V12_data[4] <- 0 
V12_data[5:11] <- V12$Proportion[c(4:10)]


combined_table <- cbind(V7$V7, V7$Proportion,V8$Proportion,V9$Proportion,V10$Proportion, V11_data, V12_data) %>% replace_na(0)
colnames(combined_table) <- c("Data observed", "V7", "V8", "V9", "V10" , "V11", "V12")

kable(combined_table, caption = "Proportion of Deafulter in PAY0, PAY2 to PAY6 (V7 to V12)") %>%
  kable_styling(latex_options = "HOLD_position")

```



## 3. Data Pre-processing

We can first split the data into 2 subsets, 1 being the `trainset`, and the other being the `testset`, in the respective proportion of 3:1. This allows us to be able to train our models effectively, and gauge the subsequent models' effectiveness on the `testset` in an unbiased manner. In addition, we will be scaling our data to ensure that all the features contribute equally to the modeling process. Scaling the dataset will help us to ensure that all features are equally weighted, making our model more accurate and robust. The process we will be scaling our data is normalisation, which rescales our data to a specific range, [0,1], depending on the minimum and maximum values of the data, ensuring that different variables with different ranges and units have a comparable scale.

```{r part3i, include = FALSE}
index <- 1:nrow(data)
testindex <- sample(index, trunc(n)/4)
test.data <- data[testindex,]
train.data <- data[-testindex,]
```


```{r part3ii, include = FALSE}
raw_train_data <- train.data
scaled.train.data <- train.data
df1 <- preProcess(scaled.train.data, method=c("range")) 
scaled.train.data <- predict(df1, as.data.frame(scaled.train.data))
#scaling test data set
raw_test_data <- test.data
scaled.test.data <- test.data
df2 <- preProcess(scaled.test.data, method=c("range")) 
scaled.test.data <- predict(df2, as.data.frame(scaled.test.data))

```


## 4. Feature Selection

As the dataset is unbalanced, where majority of the dataset comprises non-defaulters, our feature selection methods have to take the unbalanced nature of the dataset into consideration as well.

Firstly, with regards to feature selection, we can start by removing highly correlated variables. This can be done by plotting a correlation matrix and subsequently identifying which feature variables have a high correlation with one another. We can then remove such highly correlated variables to ensure that the feature variables are linearly independent from one another.

```{r part4i, include = FALSE}
# correlation matrix 
contd <- cor(scaled.train.data[c(2,3,13,14,15,16,17,18,19,20,21,22,23,24,25)])
corrplot(contd, method = "number",addCoef.col = 1, number.cex = 0.4, tl.cex = 0.4)
```

```{r part4i_graph, echo = FALSE, out.width="70%", fig.align='center'}
knitr::include_graphics("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_corr.png")
```

Based on the results of the plotted correlation matrix, we can keep 1 variable out of V13 - V18, as they are all highly correlated to one another.

Next, given that the data is unbalanced, to avoid our models producing biased results that skew towards the majority class, we can employ over/under sampling to mitigate the class imbalance issue, thereby improving the performance of our models later.


```{r part4ii, include = FALSE}
dist_before <- table(scaled.train.data$V25)

processed_balanced_train <- ROSE(V25 ~ ., data = scaled.train.data,
                  seed = 9999)$data

dist_after <- table(processed_balanced_train$V25)
```

```{r part4ii_graph, echo = FALSE}
kable(dist_before, caption = "Distribution of V25 in train set before processing the data") %>%
  kable_styling(latex_options = "HOLD_position")

kable(dist_after, caption = "Distribution of V25 after train set processing the data") %>%
  kable_styling(latex_options = "HOLD_position")

```


We can perform wrapper method feature selection (both forward and backward stepwise feature selection) to select for feature variables that are most important and useful for predicting the target variable:

```{r part4iii, include = FALSE}

# wrapper method (forward and backward stepwise)
outbackward <- regsubsets(V25 ~ 
V2 + V3 + V4 + V5 +
V6 + V7 + V8 + V9 + V10 +
V11 + V12 + V13 + V19 + V20 +
V21 + V22 + V23 + V24, data = processed_balanced_train, method = "backward")
summary(outbackward)
plot(outbackward,scale="r2", main = "Backward stepwise feature selection")

```

```{r part4iv, include = FALSE}

# wrapper method (forward and backward stepwise)

outforward <- regsubsets(V25 ~ 
V2 + V3 + V4 + V5 +
V6 + V7 + V8 + V9 + V10 +
V11 + V12 + V13 + V19 + V20 +
V21 + V22 + V23 + V24, data = processed_balanced_train, method = "forward")
summary(outforward)
plot(outforward,scale="r2", main = "Forward stepwise feature selection")
```

```{r part4iv_graph, echo = FALSE, out.width="45%", fig.align='center', fig.show = 'hold'}
knitr::include_graphics(c("/Users/jett/Desktop/Y2S2/BT2103/proj/graph_backward.png","/Users/jett/Desktop/Y2S2/BT2103/proj/graph_forward.png"))
```


Backward: V2, V6, V7, V8, V9, V10, V13, V19  

Forward: V2, V6, V7, V8, V9, V10, V13, V19


In addition, we can also perform random forest feature selection to determine the importance of the variables and choose a subset.

```{r part4v, include = FALSE}

# random forest

model_rf <- randomForest(V25 ~ 
V2 + V3 + V4 + V5 +
V6 + V7 + V8 + V9 + V10 +
V11 + V12 + V13 + V19 + V20 +
V21 + V22 + V23 + V24, data = processed_balanced_train, importance=TRUE)

model_rf

important_vars <- importance(model_rf)
important_vars %>%
  as.data.frame() %>%
  arrange(desc(`%IncMSE`))
```

```{r part4v_graph, echo = FALSE}
kable(important_vars %>%
  as.data.frame() %>%
  arrange(desc(`%IncMSE`)), caption = "Random Forest Feature Seletion") %>% kable_styling(latex_options = "HOLD_position")

```
Based on the results above of the random forest feature selection, we can take V7, V8, V9, V10, V11, V12, V19, V20.


## Model 1: Logistic Regression

Logistic regression is a statistical method used to analyse data in order to make predictions about the probability of a binary outcome. It is a computationally efficient method, making it suitable for large datasets and is able to handle both continuous and categorical predictors. This is suitable for us as it is in line with our current dataset which also have categorical and continuous features. However, the logistic regression model assumes that the predictors and outcomes have a linear relationship. In addition, it requires the independent variables are independent of each other and do not have significant outliers or suffer from multicollinearity. To help aid this model, our pre-processing has removed variables that are highly correlated from each other and irrelevant features are also eliminated during feature selection.

In this case, we are using logistic regression to predict the default status of customers in the next month. We used our processed balanced train dataset to train our model before testing it on the scaled test data. For this model, we used 2 sets of variables taken from backward-forward feature selection as well as the random forest feature selection and will be using the results to evaluate which set of variables would be selected for training the other models.

After passing in the scaled test data with parameter type "Response" to obtain a output between range 0 to 1, we plot the ROC curve to find the AUC and the k-s statistic to find the cutoff in order to find the optimal threshold value. Results above the threshold value was considered to be defaulted and below to be considered as to not have defaulted.

```{r part5iaa, echo = FALSE}

# logistic regression

set.seed(9999)

### USING VARIABLES SELECTED FROM BACKWARD AND FORWARD FEATURE SELECTION

bf.logreg <- glm(V25 ~ V2 + V6 + V7 + V8 + V9 + V10 + V13 + V19, data=processed_balanced_train, family=binomial)

summary(bf.logreg)
```

``` {r part5iab, include = FALSE}

bf.logreg

bf.logreg.pred <- predict(bf.logreg, scaled.test.data[,-25], type="response")

bf.logreg.test_result <- rocit(bf.logreg.pred, scaled.test.data$V25)
bf.logreg.auc <- bf.logreg.test_result$AUC
bf.logreg.auc #0.7195834

bf.logreg.ksplot <- ksplot(bf.logreg.test_result)
bf.logreg.ksplot$`KS Cutoff`#0.5518764


logreg.bf.results <- ifelse(bf.logreg.pred > bf.logreg.ksplot$`KS Cutoff`, 1, 0)
bf.logreg.accuracy <- mean(logreg.bf.results == scaled.test.data$V25)
bf.logreg.accuracy #0.7698667

bf.glm.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=logreg.bf.results), 2, rev)), 2, rev)

bf.glm.confusion_matrix

bf.glm.sensitivity <- bf.glm.confusion_matrix[1] / (bf.glm.confusion_matrix[1] + bf.glm.confusion_matrix[3])
bf.glm.specificity <- bf.glm.confusion_matrix[4] / (bf.glm.confusion_matrix[4] + bf.glm.confusion_matrix[2])

bf.glm.balanced_accuracy <- (bf.glm.sensitivity + bf.glm.specificity) / 2

bf.glm.balanced_accuracy #0.6904977

bf.glm.precision <- bf.glm.confusion_matrix[1] / (bf.glm.confusion_matrix[1] + bf.glm.confusion_matrix[2])
bf.glm.recall <- bf.glm.confusion_matrix[1] / (bf.glm.confusion_matrix[1] + bf.glm.confusion_matrix[3])
bf.glm.f1 <- 2 * (bf.glm.precision * bf.glm.recall) / (bf.glm.precision + bf.glm.recall)
bf.glm.average_class_accuracy <- ((bf.glm.confusion_matrix[1] / (bf.glm.confusion_matrix[1] + bf.glm.confusion_matrix[3])) + (bf.glm.confusion_matrix[2] / (bf.glm.confusion_matrix[2] + bf.glm.confusion_matrix[4]))) / 2

bf.glm.precision
bf.glm.recall
bf.glm.f1
bf.glm.average_class_accuracy

```

```{r part5ia_graph, echo = FALSE}
kable(bf.glm.confusion_matrix, caption = "Confusion Matrix for logistic regression model with backward-forward feature selection", col.names = c("1 (Predicted)", "0 (Predicted)")) %>% kable_styling(latex_options = "HOLD_position")

```

For the logistic regression model using variables selected from the backward-forward feature selection, we achieved an accuracy of 76.99%, a balanced accuracy of 69.05% and an area under ROC-curve (AUC score) of 0.7196. The confusion matrix for the model can also be observed above.

``` {r part5iba, echo=FALSE}
### USING VARIABLES SELECTED FROM RANDOM FOREST FEATURE SELECTION
set.seed(9999)

rf.logreg <- glm(V25 ~ V20 + V19 + V7 + V8 + V9 + V10 + V11 + V12, data=processed_balanced_train, family=binomial)

summary(rf.logreg)

```

``` {r part5ibb, include = FALSE}

rf.logreg

rf.logreg.pred <- predict(rf.logreg, scaled.test.data[,-25], type="response")

rf.logreg.test_result <- rocit(rf.logreg.pred, scaled.test.data$V25)
rf.logreg.auc <- rf.logreg.test_result$AUC
rf.logreg.auc #AUC = 0.7139351

rf.logreg.ksplot <- ksplot(rf.logreg.test_result)
rf.logreg.ksplot$`KS Cutoff` #0.501456

rf.logreg.results <- ifelse(rf.logreg.pred > rf.logreg.ksplot$`KS Cutoff`, 1, 0)
rf.logreg.accuracy <- mean(rf.logreg.results == scaled.test.data$V25)
rf.logreg.accuracy #0.7968


rf.glm.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=rf.logreg.results), 2, rev)), 2, rev)
rf.glm.sensitivity <- rf.glm.confusion_matrix[1] / (rf.glm.confusion_matrix[1] + rf.glm.confusion_matrix[3])
rf.glm.specificity <- rf.glm.confusion_matrix[4] / (rf.glm.confusion_matrix[4] + rf.glm.confusion_matrix[2])

rf.glm.balanced_accuracy <- (rf.glm.sensitivity + rf.glm.specificity ) / 2

rf.glm.balanced_accuracy #0.6987934

rf.glm.precision <- rf.glm.confusion_matrix[1] / (rf.glm.confusion_matrix[1] + rf.glm.confusion_matrix[2])
rf.glm.recall <- rf.glm.confusion_matrix[1] / (rf.glm.confusion_matrix[1] + rf.glm.confusion_matrix[3])
rf.glm.f1 <- 2 * (rf.glm.precision * rf.glm.recall) / (rf.glm.precision + rf.glm.recall)
rf.glm.average_class_accuracy <- ((rf.glm.confusion_matrix[1] / (rf.glm.confusion_matrix[1] + rf.glm.confusion_matrix[3])) + (rf.glm.confusion_matrix[2] / (rf.glm.confusion_matrix[2] + rf.glm.confusion_matrix[4]))) / 2

rf.glm.precision
rf.glm.recall
rf.glm.f1
rf.glm.average_class_accuracy 

```

```{r part5ib_graph, echo = FALSE}
kable(rf.glm.confusion_matrix, caption = "Confusion Matrix for logistic regression model with random forest feature selection" , col.names = c("1 (Predicted)", "0 (Predicted)")) %>% kable_styling(latex_options = "HOLD_position")

```

For the logistic regression model using variables selected from the random forest feature selection, we achieved an accuracy of 79.69%, a balanced accuracy of 69.88% and an AUC score of 0.7139. The confusion matrix for the model can also be observed above.

It can be observed that some variables such as V12, with p-values of 0.1418 which is larger than 0.05, that were selected by our random forest feature selection is not significant when predicting whether a customer will default or not. Comparatively, all variables selected by the backwards and forwards feature selection are significant, with p-values less than 0.05. In addition, the logistic regression model using variables taken from backward and forward feature selection have a higher AUC score, meaning that it is able to better distinguish between the default and non-default. Hence, we will be using the variables selected from our backward forward feature selection for the subsequent models as they are significant in predicting whether a customer will default or not.

``` {r part5iiaformula, include = FALSE}
fmla <- as.formula(as.factor(V25) ~ V2 + V6 + V7 + V8 + V9 + V10 + V13 + V19)

```

## Model 2: Support Vector Machine

A Support Vector Machine is an algorithm that can to used for classification. It finds a hyperplane that can separate the data into our 2 classes, default and non-default, and is chosen to be as far away as the closest data points from each of the classes. This model is able to work well with more complex data, but is highly sensitive to the kernel function as well as other parameters. In addition, it is computationally expensive and not efficient compared to other models.

We used a Support Vector Machine, specifically of type "C-Classification" for binary classification to predict the default status of customers in the next month. Each model will similarly be trained using our processed balanced training dataset and will then be tested on our scaled test dataset. Since SVM is very sensitive to its different parameters, we attempted different models in order to observe the difference in performance.

``` {r part5iiaa, echo = FALSE}
set.seed(9999)

#general SVM model

svm.model <- svm(fmla, data=processed_balanced_train, type="C-classification", kernel="linear")

svm.model

```

``` {r part5iiab, include = FALSE}

svm.model.pred <- predict(svm.model, scaled.test.data[,-25])

svm.model.test_result <- rocit(as.numeric(svm.model.pred), scaled.test.data$V25)
svm.model.auc <- svm.model.test_result$AUC
svm.model.auc 

accuracy_svm.model <- mean(svm.model.pred == scaled.test.data$V25)
accuracy_svm.model 

svm.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=svm.model.pred), 2, rev)), 2, rev)

svm.sensitivity <- svm.confusion_matrix[1] / (svm.confusion_matrix[1] + svm.confusion_matrix[3])
svm.specificity <- svm.confusion_matrix[4] / (svm.confusion_matrix[4] + svm.confusion_matrix[2])

svm.balanced_accuracy <- (svm.sensitivity + svm.specificity) / 2

svm.balanced_accuracy 

svm.precision <- svm.confusion_matrix[1] / (svm.confusion_matrix[1] + svm.confusion_matrix[2])
svm.recall <- svm.confusion_matrix[1] / (svm.confusion_matrix[1] + svm.confusion_matrix[3])
svm.f1 <- 2 * (svm.precision * svm.recall) / (svm.precision + svm.recall)
svm.average_class_accuracy <- ((svm.confusion_matrix[1] / (svm.confusion_matrix[1] + svm.confusion_matrix[3])) + (svm.confusion_matrix[2] / (svm.confusion_matrix[2] + svm.confusion_matrix[4]))) / 2

svm.precision
svm.recall
svm.f1
svm.average_class_accuracy 

```

```{r part5iia_graph, echo = FALSE}
kable(svm.confusion_matrix, caption = "Confusion Matrix for C-Classification SVM model", col.names = c("1 (Predicted)", "0 (Predicted")) %>% kable_styling(latex_options = "HOLD_position")

```

For the model above, we did not specify any parameters to have an estimation of the performance of a basic SVM model. We obtained an accuracy of 69.17%, a balanced accuracy of 66.86% and an AUC score of 0.6686121. The confusion matrix for this model can also be observed above.

``` {r part5iiba, echo = FALSE}

#SVM with cross-validation

svm.cross <- svm(fmla, data=processed_balanced_train, type="C-classification", kernel="linear", cross=10, cost=1)

svm.cross

```

``` {r part5iibb, include = FALSE}

svm.cross.pred <- predict(svm.cross, scaled.test.data[,-25])

svm.cross.test_result <- rocit(as.numeric(svm.cross.pred), scaled.test.data$V25)
svm.cross.auc <- svm.cross.test_result$AUC
svm.cross.auc 

svm.cross.accuracy <- mean(svm.cross.pred== scaled.test.data$V25)
svm.cross.accuracy 

svm.cross.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=svm.cross.pred), 2, rev)), 2, rev)

svm.cross.sensitivity <- svm.cross.confusion_matrix[1] / (svm.cross.confusion_matrix[1] + svm.cross.confusion_matrix[3])
svm.cross.specificity <- svm.cross.confusion_matrix[4] / (svm.cross.confusion_matrix[4] + svm.cross.confusion_matrix[2])

svm.cross.balanced_accuracy <- (svm.cross.sensitivity + svm.cross.specificity) / 2

svm.cross.balanced_accuracy 

svm.cross.precision <- svm.cross.confusion_matrix[1] / (svm.cross.confusion_matrix[1] + svm.cross.confusion_matrix[2])
svm.cross.recall <- svm.cross.confusion_matrix[1] / (svm.cross.confusion_matrix[1] + svm.cross.confusion_matrix[3])
svm.cross.f1 <- 2 * (svm.cross.precision * svm.cross.recall) / (svm.cross.precision + svm.cross.recall)
svm.cross.average_class_accuracy <- ((svm.cross.confusion_matrix[1] / (svm.cross.confusion_matrix[1] + svm.cross.confusion_matrix[3])) + (svm.cross.confusion_matrix[2] / (svm.cross.confusion_matrix[2] + svm.cross.confusion_matrix[4]))) / 2

svm.cross.precision
svm.cross.recall
svm.cross.f1
svm.cross.average_class_accuracy 
```

```{r part5iib_graph, echo = FALSE}
kable(svm.cross.confusion_matrix, caption = "Confusion Matrix for C-Classification with 10-fold cross validation SVM model", col.names = c("1 (Predicted)", "0 (Predicted)")) %>% kable_styling(latex_options = "HOLD_position")

```

To improve the first model, we performed 10-fold cross validation to prevent overfitting and allow the model to be a more generalised one. For this model, We obtained a similar accuracy of 69.17%, a balanced accuracy of 66.86% and an AUC score of 0.6686121. The confusion matrix for this model can also be observed above.


``` {r part5iica, echo=FALSE}
#SVM with cross-validation and weights that give a larger penalty for wrongly classifying customers who default as customers who do not default

class_counts <- table(processed_balanced_train$V25)
class_weights <- 1 / prop.table(class_counts)

svm.weighted <- svm(fmla, data=processed_balanced_train, type="C-classification", kernel="linear", cross=10, cost=1, class.weights=class_weights)

svm.weighted

```

``` {r part5iicb, include = FALSE}

svm.weighted.pred <- predict(svm.weighted, scaled.test.data[,-25])

svm.weighted.test_result <- rocit(as.numeric(svm.weighted.pred), scaled.test.data$V25)
svm.weighted.auc <- svm.weighted.test_result$AUC
svm.weighted.auc 

svm.weighted.accuracy<- mean(svm.weighted.pred == scaled.test.data$V25)

svm.weighted.accuracy

svm.weighted.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=svm.weighted.pred), 2, rev)), 2, rev)

svm.weighted.sensitivity <- svm.weighted.confusion_matrix[1] / (svm.weighted.confusion_matrix[1] + svm.weighted.confusion_matrix[3])
svm.weighted.specificity <- svm.weighted.confusion_matrix[4] / (svm.weighted.confusion_matrix[4] + svm.weighted.confusion_matrix[2])

svm.weighted.balanced_accuracy <- (svm.weighted.sensitivity + svm.weighted.specificity) / 2

svm.weighted.balanced_accuracy 

svm.weighted.precision <- svm.weighted.confusion_matrix[1] / (svm.weighted.confusion_matrix[1] + svm.weighted.confusion_matrix[2])
svm.weighted.recall <- svm.weighted.confusion_matrix[1] / (svm.weighted.confusion_matrix[1] + svm.weighted.confusion_matrix[3])
svm.weighted.f1 <- 2 * (svm.weighted.precision * svm.weighted.recall) / (svm.weighted.precision + svm.weighted.recall)
svm.weighted.average_class_accuracy <- ((svm.weighted.confusion_matrix[1] / (svm.weighted.confusion_matrix[1] + svm.weighted.confusion_matrix[3])) + (svm.weighted.confusion_matrix[2] / (svm.weighted.confusion_matrix[2] + svm.weighted.confusion_matrix[4]))) / 2

svm.weighted.precision
svm.weighted.recall
svm.weighted.f1
svm.weighted.average_class_accuracy 

```

```{r part5iic_graph, echo = FALSE}
kable(svm.weighted.confusion_matrix, caption = "Confusion Matrix for C-Classification with 10-fold cross validation and weighted SVM model", col.names = c("1 (Predicted)", "0 (Predicted)")) %>% kable_styling(latex_options = "HOLD_position")

```

To further improve the previous model, we added class weights to penalise customers who default by placing more emphasis on detecting customers who are likely to default. For this model, We obtained an accuracy of 70.74%, a balanced accuracy of 67.31% and an AUC score of 0.6731436. The confusion matrix for this model can also be observed above.

### Neural Network Model

Neural network models is also computationally expensive and can be prone to overfitting, depending on the hyperparameters inputted for the model, but they are able to capture complex nonlinear relationships in the data where during model training, the model adjusts the weights of the connections between its neurons to minimize the difference between the predicted and actual values. 

As such, with our set of selected variables, we can run a neural network model in order to generate an accurate predictive classification of defaulters and non-defaulters.

```{r part5iii, include = FALSE}

# neural net model
set.seed(9999)

nn <- nnet(fmla,data=processed_balanced_train,maxit=1000,size=20, decay=0.2)

nn

nn.pred <- predict(nn, newdata = scaled.test.data[,-25], type = c("class"))

nn.test_result <- rocit(as.numeric(nn.pred), scaled.test.data$V25)
nn.auc <- nn.test_result$AUC 
nn.auc 

nn.accuracy <- mean(scaled.test.data$V25 == nn.pred) 
nn.accuracy 

nn.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=nn.pred), 2, rev)), 2, rev)
nn.sensitivity <- nn.confusion_matrix[1] / (nn.confusion_matrix[1] + nn.confusion_matrix[3])
nn.specificity <- nn.confusion_matrix[4] / (nn.confusion_matrix[4] + nn.confusion_matrix[2])

nn.balanced_accuracy <- (nn.sensitivity  + nn.specificity) / 2

nn.balanced_accuracy 

nn.precision <- nn.confusion_matrix[1] / (nn.confusion_matrix[1] + nn.confusion_matrix[2])
nn.recall <- nn.confusion_matrix[1] / (nn.confusion_matrix[1] + nn.confusion_matrix[3])
nn.f1 <- 2 * (nn.precision * nn.recall) / (nn.precision + nn.recall)
nn.average_class_accuracy <- ((nn.confusion_matrix[1] / (nn.confusion_matrix[1] + nn.confusion_matrix[3])) + (nn.confusion_matrix[2] / (nn.confusion_matrix[2] + nn.confusion_matrix[4]))) / 2

nn.precision
nn.recall
nn.f1
nn.average_class_accuracy

```

```{r part5iii_graph, echo = FALSE}
kable(nn.confusion_matrix, caption = "Confusion Matrix for neural network model", col.names = c("1 (Predicted)", "0 (Predicted)")) %>% kable_styling(latex_options = "HOLD_position")

```

Above shows the confusion matrix based on the prediction done by the neural network model.

### Random Forest Model

As the random forest model constructs and combines decision trees in order to generate its predictive model, it greatly reduces the chance of model overfitting. Furthermore, due to its ability to handle high dimension datasets like the one used, as well as its robust capacity to handle outliers, we decided to run the random forest model as well.

```{r part5iv, include = FALSE}

# random forest model
set.seed(9999)
rf.model <- randomForest(fmla, data = processed_balanced_train)

rf.pred <- predict(rf.model, scaled.test.data)

rf.result <- rocit(as.numeric(rf.pred), scaled.test.data$V25)
rf.auc <- rf.result$AUC
rf.auc 

rf.accuracy <- mean(scaled.test.data$V25 == rf.pred)
rf.accuracy

rf.confusion_matrix <- apply(t(apply(table(actual=scaled.test.data$V25, pred=rf.pred), 2, rev)), 2, rev)
rf.sensitivity <- rf.confusion_matrix[4] / (rf.confusion_matrix[4] + rf.confusion_matrix[2])
rf.specificity <- rf.confusion_matrix[1] / (rf.confusion_matrix[1] + rf.confusion_matrix[3])

rf.balanced_accuracy <- (rf.sensitivity  + rf.specificity) / 2

rf.balanced_accuracy 

rf.precision <- rf.confusion_matrix[1] / (rf.confusion_matrix[1] + rf.confusion_matrix[2])
rf.recall <- rf.confusion_matrix[1] / (rf.confusion_matrix[1] + rf.confusion_matrix[3])
rf.f1 <- 2 * (rf.precision * rf.recall) / (rf.precision + rf.recall)
rf.average_class_accuracy <- ((rf.confusion_matrix[1] / (rf.confusion_matrix[1] + rf.confusion_matrix[3])) + (rf.confusion_matrix[2] / (rf.confusion_matrix[2] + rf.confusion_matrix[4]))) / 2

rf.precision
rf.recall
rf.f1
rf.average_class_accuracy 
```

```{r part5iv_graph, echo = FALSE}
kable(rf.confusion_matrix, caption = "Confusion Matrix for random forest model", col.names = c("1 (Predicted)", "0 (Predicted)")) %>% kable_styling(latex_options = "HOLD_position")

```

Above illustrates the confusion matrix done by the random forest model.

## 6. Model Evaluation

For our model evaluation, we will be using Default (1) as our positive instance and Non-Default(0) as our negative instance for our calculations.

```{r part6, include = FALSE}

model_results <- data.frame(Model = c("Logistic Regression", "SVM", "SVM w/ K-fold", "SVM w/ weighted K-fold", "Neural Network", "Random Forest"),
                             Acc = c(rf.logreg.accuracy, svm.weighted.accuracy, svm.cross.accuracy, svm.weighted.accuracy, nn.accuracy, rf.accuracy),
                            Balance_Acc= c(rf.glm.balanced_accuracy, svm.weighted.balanced_accuracy, svm.cross.balanced_accuracy, svm.weighted.balanced_accuracy, nn.balanced_accuracy, rf.balanced_accuracy),
                             Avg_Class_Acc = c(rf.glm.average_class_accuracy, svm.average_class_accuracy, svm.cross.average_class_accuracy, svm.weighted.average_class_accuracy, nn.average_class_accuracy, rf.average_class_accuracy),
                             Recall = c(rf.glm.recall, svm.recall, svm.cross.recall, svm.weighted.recall, nn.recall, rf.recall),
                             Precision = c(rf.glm.precision, svm.precision, svm.cross.precision, svm.weighted.precision, nn.precision, rf.precision),
                             F1_Score = c(rf.glm.f1, svm.f1, svm.cross.f1, svm.weighted.f1, nn.f1, rf.f1),
                             AUC = c(rf.logreg.auc, svm.model.auc, svm.cross.auc, svm.weighted.auc, nn.auc, rf.auc))

```

```{r part6_graph, echo = FALSE}
kable(model_results, caption = "Model Performance Metrics") %>% kable_styling(latex_options = "HOLD_position", full_width = T, font_size = 10)
```

We will be able to make a more in-depth evaluation of the models through the various metrics obtained. Specifically, we can gauge the models' performances through `Accuracy`, `Balanced Accuracy`, `Average Class Accuracy`, `Precision`, `Recall`, `F1` and `Area Under ROC Curve`.

However, it should be worth noting that since the dataset is very imbalanced in nature, the `Accuracy` metric should not be used as a good gauge of a model's performance. Instead, `Balanced Accuracy` will be a better metric to use, because it considers the accuracy of each class separately and subsequently averages them such that the accuracy is weighted by the classes. This can be illustrated as a trivial model with all instances predicted as non-default would have an accuracy of 77.8% but would only have a balanced accuracy of 50%. Since all our models have a balanced accuracy greater than 50%, we would be considering all models as valid for the bank.

Based on the `Area Under ROC Curve` as well as `Balanced Accuracy` metrics, the Logistic Regression model performs the best, where it was the best in distinguishing and classifying between default and non-default observations. The performance of the Logistic Regression model with regards to `Area Under ROC Curve` and `Balanced Accuracy` meant that the Logistic Regression model was the best at distinguishing and subsequently classifying between defaulters and non-defaulters. 

The `Average Class Accuracy` metric, which measures the average accuracy per class, is useful given that the dataset is highly imbalanced. With regards to this metric, the `Neural Network` model performs the best, indicating that considering the imbalanced nature of the dataset, the Neural Network model was the most accurate at correctly identifying and classifying defaulters and non-defaulters.

The Neural Network model was also the best at the `Recall` and `F1 Score` metric. This meant that it was best able to reduce the number of misclassifications of defaulters as non-defaulters. As `Recall` measures the number of correct predictions across all positive instances, the Neural Network model was the most capable at correctly classifying and identifying defaulters. Additionally, with regards to the `F1 Score`, since it is a harmonic mean of both `Precision` and `Recall`, this means that the Neural Network achieved the best balance between `Precision` and `Recall`, and is overall well-performing in both aspects.

With regards to `Precision`, however, both the SVM model with K-Fold Cross Validation and the SVM model with Weighted K-Fold Cross Validation performed the best, which indicates that these 2 models were the best at preventing the misclassification of non-defaulters as defaulters.


From the perspective of a bank, there are 2 key considerations for the bank:

1) Minimise the number of defaulters misclassified as non-defaulters

2) Minimise the number of non-defaulters misclassified as defaulters.

For consideration 1, this is particularly significant as misclassifying defaulters as non-defaulters would pose a large financial risk on the bank when issuing loans, especially when the bank is tight on cash as the borrower might delay repayments. As such, the `Recall` metric should be a key consideration of the bank when gauging model performance, which means that the Neural Network model should be the most relevant model for the bank to use when making predictive classifications of its customers, as it would greatly reduce the amount of risk that the bank encounters.

For consideration 2, this is also important as misclassifying non-defaulters as defaulters would mean that the bank would lose out on a group of potential loan customers and by extension, losing out on potential revenue. As such the `Precision` metric should be a key consideration of the bank when evaluating model performance, which would indicate that either the SVM Model with K-Fold Cross Validation or the SVM model with weighted K-Fold Cross Validation should be used in order to minimise the number of non-defaulters incorrectly classified as defaulters.

Overall, however, due to the strong performance of the Neural Network model in the `F1 Score` metric, it should be the primary model that the bank uses to evaluate its customers' risk of default, as it offers great performance in both `Precision` and `Recall`, though it underperforms in terms of correctly distinguishing between defaulters and non-defaulters (`Area Under ROC Curve`) compared to the Logistic Regression.

## 7. Possible Improvements

### General

Instead of splitting the dataset into only train and test, we should experiment splitting into 3 sub-categories: train, validation and test. By using a separate validation set, we can evaluate the model's performance on a dataset that was not used during training and ensure that the model generalizes well to new data. Moreover, we can also use this validation set to tune the model's hyperparameters to improve its performance. The test set would then be used to evaluate the final performance of the model because it provides an unbiased estimate of the model's performance on new data that was not used during training. This overall helps to reduce the chance of overfitting and further improve our models' performance on actual data.

### Support Vector Model

Tuning of the parameters such as choosing a different kernel, as the class may not be linearly separable and using a polynomial kernel may provide a more accurate model. Other parameters such as `cost`, `class.weights` and `margin` can also be modified and tested with different thresholds to possibly obtain a more accurate and performance model.

### Neural Network

We can increase the number of hidden layers to the neural network in order to capture the benefits of using a deep-learning learning network. By adding more layers to our model, it increases the ability to learn the complex patterns in the data and make more accurate predictions. This is done as the network perform more complex transformations on the input data, allowing it to learn more intricate patterns. However, adding too many layers will lead to overfitting. Hence, we should increase the number of hidden layers by a suitable amount to obtain a more accurate model while not over-fitting. The use of validation set mentioned previously in order to validate the number of layers before using the model on the test set can help further improve the model.

### Random Forest and Neural Network

In addition, we can also use hyperparameter tuning methods such as Grid Search or Random Search in order to find the optimal hyperparameters so that our Neural Network and Random Forest Models can be further optimised. However, due to the dimensionality of the dataset, the performance optimisations through these methods will take a long time to evaluate, and will possibly require greater computing power.












